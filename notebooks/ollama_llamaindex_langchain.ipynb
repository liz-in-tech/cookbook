{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e41275b-0b18-4203-bdeb-403d912298ff",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "- 1. llamaindex + ollama\n",
    "    - 1.1. Ollama\n",
    "    - 1.2. OllamaEmbedding\n",
    "- 2. langchain + ollama\n",
    "    - 2.1. OllamaLLM\n",
    "    - 2.2. OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36da8a-83db-4de2-a8d3-6a31627defed",
   "metadata": {},
   "source": [
    "## 1. llamaindex + ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b363f-ca0e-4d06-bc93-405989f3ba61",
   "metadata": {},
   "source": [
    "### 1.1. Ollama\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/api_reference/llms/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e06245-46ac-48b6-9f73-ac5536b0192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effa2e47-630e-431e-801a-364dd66d09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"no_proxy\"] = \"127.0.0.1,localhost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb6956d-67a6-43cd-a89c-b6c205986ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = Ollama(model=\"llama3\", request_timeout=60.0)\n",
    "\n",
    "# changing the global default\n",
    "Settings.llm = llm\n",
    "\n",
    "response = llm.complete(\"What is the capital of France?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7cd4cb-039e-48d5-be40-fb08837692c6",
   "metadata": {},
   "source": [
    "### 1.2. OllamaEmbedding\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/examples/embeddings/ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc0016e-b9b3-42c2-8fac-66ed61fce792",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0164c93-14ba-4f9b-8368-81a7c6c82f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    "    request_timeout=660.0\n",
    ")\n",
    "\n",
    "# changing the global default\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8563fd4-1ef2-429b-9fc1-0bc29660c210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3424331247806549, 0.38466677069664, -2.389650344848633, -0.3208850026130676, 1.0093443393707275, \n"
     ]
    }
   ],
   "source": [
    "query_embedding = ollama_embedding.get_query_embedding(\"Where is blue?\")\n",
    "\n",
    "print(str(query_embedding)[:100]) # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a211a316-fbc8-4788-a3e1-8e3f9da464d0",
   "metadata": {},
   "source": [
    "## 2. langchain + ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4677e8a3-c91d-45cb-8004-e33b97e24650",
   "metadata": {},
   "source": [
    "### 2.1. OllamaLLM\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df29b83f-b5dd-4537-be23-236c4ea652d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce385d0-c840-444a-bbb0-1b2efc938402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"no_proxy\"] = \"127.0.0.1,localhost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa39d34-8a02-4e3d-9b20-f44d051fbd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75032a44-32ea-45fa-b3da-073aadb6731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A thoughtful and analytical approach! LangChain, you ask? Well, let me break it down for you...\n",
      "\n",
      "LangChain is a type of generative language model designed to generate human-like text responses to given prompts or questions. It's a deep learning-based AI that can produce coherent and natural-sounding text in various styles, formats, and tones.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. **Training**: LangChain is trained on massive datasets of text from the internet, books, articles, and other sources.\n",
      "2. **Prompt understanding**: When you provide a prompt or question, LangChain uses its understanding of language patterns, grammar, and semantics to comprehend what you're asking.\n",
      "3. **Response generation**: LangChain generates a response based on its training data, taking into account the context, tone, and style required for the specific prompt.\n",
      "\n",
      "LangChain's capabilities include:\n",
      "\n",
      "* Generating creative writing, such as stories or poems\n",
      "* Answering questions or providing information on various topics\n",
      "* Summarizing long texts or articles\n",
      "* Creating chatbot-like conversations\n",
      "\n",
      "By using natural language processing (NLP) and machine learning techniques, LangChain can produce highly realistic and engaging text responses that might even fool some humans into thinking they were written by a human!\n",
      "\n",
      "So, that's LangChain in a nutshell! What do you think about this AI-powered language model?\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c6a0eb-4770-44ee-9af9-e6ce559a3929",
   "metadata": {},
   "source": [
    "### 2.2. OllamaEmbeddings\n",
    "\n",
    "https://python.langchain.com/docs/integrations/text_embedding/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91389477-edd1-4e12-8ce7-a2837d98e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9128bec6-e0b8-413b-bd0b-919fb338a243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01694864, 0.012009434, -0.11784942, -0.10173338, 0.05472736, -0.088820376, -0.017708793, 0.009860\n",
      "[-0.026370758, 0.036837026, -0.162758, -0.09021651, -0.022131497, -0.09518427, -0.03739478, 0.005037\n"
     ]
    }
   ],
   "source": [
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "text2 = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    ")\n",
    "two_vectors = embeddings.embed_documents([text, text2])\n",
    "\n",
    "for vector in two_vectors:\n",
    "    print(str(vector)[:100])  # Show the first 100 characters of the vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
